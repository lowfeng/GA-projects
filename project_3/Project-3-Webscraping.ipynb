{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12e4d441",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Project 3 - Classifying Xbox and Playstation Subreddits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985d89c3",
   "metadata": {},
   "source": [
    "## Content\n",
    "\n",
    "Project will be split in 3 seperate workbooks for organisation\n",
    "\n",
    "* **Webscraping**\n",
    "* **Data Cleaning EDA Feature Selection**\n",
    "* **Model Selection**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1357b22",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Reddit has been a centralized source of gaming news, opinions and debates for consoles and their games.  In particular, the [r/xbox](https://www.reddit.com/r/xbox/) and [r/playstation](https://www.reddit.com/r/playstation/) consoles are the 2 most popular and are always in comparisons with each other. \n",
    "\n",
    "As a consumer, buying a console on which one is the most popular, may seem like a poor choice to make.\n",
    "\n",
    "But, thanks to the psychological concept of informational social influence, it is one many gamers make. What this means is that many consumers will look towards their peers online to choose the ‘right’ console because of a desire to be correct when there is no obvious choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f30af22",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "The moderators of the xbox and playstation subreddits understand that for the consumer to make a proper informed decision, ideally their subreddits should only contain posts related to their console.\n",
    "\n",
    "As misclassification of consoles by the individual subreddit authors (intentionally/mistakenly), are confusing the readers on the subject topic. \n",
    "\n",
    "As such, the moderators would like to identify posts in their subreddit that are about their rival console. Hence we have been tasked to construct and find the best classification model to correctly predict the subreddit a given post belongs to, through identifing the keywords associated with the subreddit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600af05c",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "* [api.pushshift.io](https://api.pushshift.io/reddit/submission/search):  We will be using this program to perform webscraping on the 2 subreddit posts\n",
    "* [`xbox.csv`](./datasets/xbox.csv): This contains the most recent 2200 xbox subreddit posts\n",
    "* [`playstation.csv`](./datasets/playstation.csv): This contains the most recent 2200 playstation subreddit posts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec23984",
   "metadata": {},
   "source": [
    "# Outside Research\n",
    "\n",
    "* According to a [comment](https://www.reddit.com/r/xboxone/comments/iwour0/unpopular_opinion_we_need_to_unify_all_xbox/) made on the xbox subreddit, there are many subreddits made for each generation of console (r/XboxSeriesX, r/SeriesXbox, r/XboxOne, r/PS4, r/PS5 etc.) instead of one subreddit for each console type where the community can engage in. For the consumer looking to buy a console, they probably will explore beyond r/xbox and r/playstation and go to more specific subreddits on next-gen consoles. Ideally for our model, scraping from these subreddits would improve our model.\n",
    "\n",
    "* The subreddits contains posts by the Moderator and bots that inform users on the ethics and rules of the subreddit. These are repetitive posts and do not contain any relevant information for building our model.\n",
    "\n",
    "* If there are NSFW (Not Safe For Work) posts, its text is hidden by fault. This could lead to us scraping an empty post\n",
    "\n",
    "* Reddit includes an API that allows us to send a request and receive a JSON containing information about a post, but there are issues with this. One being that reddit throttles requests to once every few seconds, which we will deal with by puting a sleep statement in our code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c27ee31",
   "metadata": {},
   "source": [
    "# Data Dictionary\n",
    "\n",
    "Data contains the most recent 2200 xbox and playstation subreddit posts each, with the below table showing the variables of interest\n",
    "\n",
    "\n",
    "|Variables|Dataset|Type|Description|  \n",
    "|:--|:-:|:-:|:--|  \n",
    "|author|unstructured|object|Subredditor whom posted submission|  \n",
    "|title|unstructured|object|Title of the post|  \n",
    "|self_text|unstructured|object|Includes all text inside the post|  \n",
    "|subreddit|structured|object|which subreddit the post was classified under|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0a392a",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e07df65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfb534b",
   "metadata": {},
   "source": [
    "# Webscraping from Reddit\n",
    "\n",
    "A function was created to scrape the posts in reddit as the api allowed for us to collect 100 at a time. Ideally, we want to have enough posts for modelling (ideally a 1000 per subreddit), but we will be conservative and collect 2200 for each subreddit to account for posts that have missing values or removed during EDA (bot or moderator generated which won't be useful to our model etc.)\n",
    "\n",
    "Essentially, this is the workflow of the loop function to get 100 posts each time\n",
    "* request data from url\n",
    "* convert data to a JSON string \n",
    "* store JSON string into a new list\n",
    "* convert list to a dataframe\n",
    "* concatenate dataframes\n",
    "* write dataframe to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f337d7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to get posts from subreddit\n",
    "def get_posts(subreddit, loops):\n",
    "\n",
    "    url = 'https://api.pushshift.io/reddit/submission/search'\n",
    "    dfs = []\n",
    "    start_time = time.time()\n",
    "    params = {\n",
    "        'subreddit': subreddit,\n",
    "        'size': 100,\n",
    "        'before': round(start_time)\n",
    "        }\n",
    "    \n",
    "    for i in range(loops):\n",
    "        current_time = time.time()\n",
    "        \n",
    "        # Request data\n",
    "        res = requests.get(url, params)\n",
    "        print(f'res {i+1} code: ', res.status_code)\n",
    "        data = res.json()\n",
    "        posts = data['data']\n",
    "        post_df = pd.DataFrame(posts)\n",
    "        dfs.append(post_df)\n",
    "        \n",
    "        # Get oldest post time and use as before parameter in next request\n",
    "        oldest = post_df['created_utc'].min()\n",
    "        params['before'] = oldest\n",
    "        \n",
    "        # Suspend execution for 1 second\n",
    "        time.sleep(1)\n",
    "    reddit_posts = pd.concat(dfs)\n",
    "\n",
    "    reddit_posts.to_csv('./datasets/' + subreddit + 'test.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51b62494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res 1 code:  200\n",
      "res 2 code:  200\n",
      "res 3 code:  200\n",
      "res 4 code:  200\n",
      "res 5 code:  200\n",
      "res 6 code:  200\n",
      "res 7 code:  200\n",
      "res 8 code:  200\n",
      "res 9 code:  200\n",
      "res 10 code:  200\n",
      "res 11 code:  200\n",
      "res 12 code:  200\n",
      "res 13 code:  200\n",
      "res 14 code:  200\n",
      "res 15 code:  200\n",
      "res 16 code:  200\n",
      "res 17 code:  200\n",
      "res 18 code:  200\n",
      "res 19 code:  200\n",
      "res 20 code:  200\n",
      "res 21 code:  200\n",
      "res 22 code:  200\n"
     ]
    }
   ],
   "source": [
    "# Call function to get xbox subreddit posts\n",
    "get_posts('xbox', 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "052acb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res 1 code:  200\n",
      "res 2 code:  200\n",
      "res 3 code:  200\n",
      "res 4 code:  200\n",
      "res 5 code:  200\n",
      "res 6 code:  200\n",
      "res 7 code:  200\n",
      "res 8 code:  200\n",
      "res 9 code:  200\n",
      "res 10 code:  200\n",
      "res 11 code:  200\n",
      "res 12 code:  200\n",
      "res 13 code:  200\n",
      "res 14 code:  200\n",
      "res 15 code:  200\n",
      "res 16 code:  200\n",
      "res 17 code:  200\n",
      "res 18 code:  200\n",
      "res 19 code:  200\n",
      "res 20 code:  200\n",
      "res 21 code:  200\n",
      "res 22 code:  200\n"
     ]
    }
   ],
   "source": [
    "# Call function to get playstation subreddit posts\n",
    "get_posts('playstation', 22)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
